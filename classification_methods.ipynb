{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First idea : label words as important if they are in the title \n",
    "\n",
    "The goal of the two ideas here is to present methods that transform the generation problem into classification problem (see report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoding = 'utf-8'\n",
    "data_train_ = []\n",
    "with open('data/train.csv', 'r', encoding=data_encoding) as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        data_train_.append(row)\n",
    "\n",
    "data_test_ = []\n",
    "with open('data/validation.csv', 'r', encoding=data_encoding) as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        data_test_.append(row)\n",
    "\n",
    "data_train = data_train_[1:]\n",
    "data_test = data_test_[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('french')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() # Mettre en minuscule\n",
    "    newtext = []\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            newtext.append(word)\n",
    "    text = ' '.join(newtext)\n",
    "    text = re.sub(r'\\(.*\\)','', text) # Supprimer les parenthèses et leur contenu\n",
    "    text = re.sub(r'[^a-zA-Z0-9 àâäéèêëîïôöùûüç]', ' ', text) # Supprimer les ponctuations sauf les points\n",
    "    text = re.sub(r'\\s+', ' ', text) # Supprimer les espaces multiples\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_preprocessed = [(preprocess(x[0]), preprocess(x[1])) for x in data_train]\n",
    "data_test_preprocessed = [(preprocess(x[0]), preprocess(x[1])) for x in data_test]\n",
    "\n",
    "MAX_LENGTH_input = 300\n",
    "\n",
    "for i in range(len(data_train_preprocessed)):\n",
    "    x, y = data_train_preprocessed[i]\n",
    "    word = x.split()[:MAX_LENGTH_input- 2]\n",
    "    word = ['SOS'] + word + ['EOS']\n",
    "    data_train_preprocessed[i] = (' '.join(word), y)\n",
    "\n",
    "print(data_train_preprocessed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "UNK_token = 3\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOS\": 0, \"EOS\": 1, \"PAD\": 2, \"UNK\": 3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\", 3: \"UNK\"}\n",
    "        self.word2count[\"SOS\"] = 1000000000\n",
    "        self.word2count[\"EOS\"] = 100000000\n",
    "        self.word2count[\"PAD\"] = 10000000\n",
    "        self.word2count[\"UNK\"] = 1000000\n",
    "        self.n_words = 4  # Count SOS and EOS and PAD and UNK\n",
    "        self.threshold_input = 10\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def smaller_vocab(self, threshold):\n",
    "        self.word2index = {k : v for k,v in self.word2index.items() if self.word2count[k] > threshold}\n",
    "        #sort\n",
    "        self.word2index = dict(sorted(self.word2index.items(), key=lambda item: item[1]))\n",
    "        self.index2word = {v : k for k,v in self.word2index.items()}\n",
    "        self.n_words = len(self.word2index.keys())\n",
    "\n",
    "def readLangs(text):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    input_lang = Lang(text)\n",
    "\n",
    "    return input_lang\n",
    "\n",
    "def prepareData(texts):\n",
    "    input_lang = readLangs(texts)\n",
    "    print(\"Counting words...\")\n",
    "    for text in texts:\n",
    "        input_lang.addSentence(text)\n",
    "    \n",
    "    input_lang.smaller_vocab(input_lang.threshold_input)\n",
    "    return input_lang\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ') if word in lang.word2index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x[0] for x in data_train_preprocessed]\n",
    "summary = [x[1] for x in data_train_preprocessed]\n",
    "\n",
    "input_lang = prepareData(x)\n",
    "\n",
    "print(input_lang.n_words)\n",
    "print(data_train_preprocessed[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_index_x = [indexesFromSentence(input_lang, x[0]) for x in data_train_preprocessed]\n",
    "\n",
    "for i in range(len(data_train_index_x)):\n",
    "    for j in range(len(data_train_index_x[i])):\n",
    "        if data_train_index_x[i][j] >= input_lang.n_words:\n",
    "            data_train_index_x[i][j] = UNK_token\n",
    "\n",
    "#padding\n",
    "for i in range(len(data_train_index_x)):\n",
    "    data_train_index_x[i] = data_train_index_x[i] + [PAD_token] * (MAX_LENGTH_input - len(data_train_index_x[i]))\n",
    "\n",
    "y = []\n",
    "for i in range(len(data_train_index_x)):\n",
    "    classification = []\n",
    "    words = [input_lang.index2word[x] for x in data_train_index_x[i]]\n",
    "    for word in words:\n",
    "        if word in summary[i]:\n",
    "            classification.append(1)\n",
    "        else:\n",
    "            classification.append(0)\n",
    "    y.append(classification)\n",
    "\n",
    "print(len(data_train_index_x[0]))\n",
    "print(len(y[0]))\n",
    "\n",
    "data_train_index_x_t = torch.tensor(data_train_index_x, dtype=torch.long, device=device)\n",
    "data_train_index_y_t = torch.tensor(y, dtype=torch.float, device=device)\n",
    "\n",
    "trainset = data.TensorDataset(data_train_index_x_t, data_train_index_y_t)\n",
    "trainloader = data.DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.2):\n",
    "        super(WordClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, bidirectional=True)  # Using bidirectional GRU\n",
    "        self.fc1 = nn.Linear(hidden_dim*2, hidden_dim)  # Doubling hidden_dim due to bidirectional\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        hidden = self.dropout(output[:, -1, :])\n",
    "        output = F.relu(self.fc1(hidden))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordClassifier(input_lang.n_words, 30, 20, MAX_LENGTH_input).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #clamp\n",
    "            outputs = torch.clamp(outputs, 0, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print('[%d] loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, trainloader, optimizer, criterion, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tensor, input_lang):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor)\n",
    "    return output\n",
    "\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs, labels = data\n",
    "    outputs = evaluate(model, inputs, input_lang)\n",
    "    print(outputs)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second idea : label sentences as important if they are the nearest (in term of rouge score) from the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "data_encoding = 'utf-8'\n",
    "data_train_ = []\n",
    "with open('data/train.csv', 'r', encoding=data_encoding) as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        data_train_.append(row)\n",
    "\n",
    "data_test_ = []\n",
    "with open('data/validation.csv', 'r', encoding=data_encoding) as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        data_test_.append(row)\n",
    "\n",
    "data_train = data_train_[1:]\n",
    "data_test = data_test_[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'])\n",
    "\n",
    "def find_best_sentence(data_train):\n",
    "    text = data_train[0]\n",
    "    title = data_train[1]\n",
    "    sentences = re.split(r'[.]', text)\n",
    "    best_sentence = ''\n",
    "    best_score = 0\n",
    "    idx = 0\n",
    "    for i in range(len(sentences)):\n",
    "        score = scorer.score(sentences[i], title)['rougeL'][2]\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_sentence = sentences[i]\n",
    "            idx = i\n",
    "    return best_sentence, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Charger un modèle pré-entraîné pour l'embedding de phrases\n",
    "model_encodding = SentenceTransformer('distiluse-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'])\n",
    "\n",
    "def find_best_sentence(data_train):\n",
    "    text = data_train[0]\n",
    "    title = data_train[1]\n",
    "    sentences = re.split(r'[.]', text)\n",
    "    best_sentence = ''\n",
    "    best_score = 0\n",
    "    idx = 0\n",
    "    for i in range(len(sentences)):\n",
    "        score = scorer.score(sentences[i], title)['rougeL'][2]\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_sentence = sentences[i]\n",
    "            idx = i\n",
    "    return best_sentence, idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i in range(len(data_train[:10000])):\n",
    "    if i % 100 == 0:\n",
    "        print('Processing', i, 'of', len(data_train))\n",
    "    sentences = re.split(r'[.]', data_train[i][0])\n",
    "    best_sentence, idx = find_best_sentence(data_train[i])\n",
    "    text_encode = model_encodding.encode(data_train[i][0])\n",
    "    for j in range(len(sentences)):\n",
    "        if j == idx:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        sentence_encode = model_encodding.encode(sentences[j])\n",
    "        x.append([text_encode, sentence_encode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = torch.tensor(x)\n",
    "y_t = torch.tensor(y)\n",
    "\n",
    "dataset = data.TensorDataset(x_t, y_t)\n",
    "dataloader = data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model that takes the sentences and the text and return the probability that the sentence is the best one\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.cnn_text = nn.Conv1d(3, 128, 3)\n",
    "        self.cnn_sentence = nn.Conv1d(3, 128, 3)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, text, sentence):\n",
    "        #repeter 3 fois text en dimension 1\n",
    "        text = text.unsqueeze(1)\n",
    "        text = text.repeat(1, 3, 1)\n",
    "        sentence = sentence.unsqueeze(1)\n",
    "        sentence = sentence.repeat(1, 3, 1)\n",
    "        text = self.cnn_text(text)\n",
    "        sentence = self.cnn_sentence(sentence)\n",
    "        text = F.max_pool1d(text, text.size(2)).squeeze(2)\n",
    "        sentence = F.max_pool1d(sentence, sentence.size(2)).squeeze(2)\n",
    "        x = torch.cat((text, sentence), 1)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        text = x[:, 0].to(device)\n",
    "        sentence = x[:, 1].to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text, sentence)\n",
    "        loss = criterion(output, y.float().view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def test_validation(model, data_test, device):\n",
    "    model.eval()\n",
    "    mean_score = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(1000):\n",
    "            sentences = re.split(r'[.]', data_test[i][0])\n",
    "            text_encode = model_encodding.encode(data_test[i][0])\n",
    "            text_encode = torch.tensor(text_encode).to(device)\n",
    "            text_encode = text_encode.unsqueeze(0)\n",
    "            predictions = []\n",
    "            for j in range(len(sentences)):\n",
    "                sentence_encode = model_encodding.encode(sentences[j])\n",
    "                sentence_encode = torch.tensor(sentence_encode).to(device)\n",
    "                sentence_encode = sentence_encode.unsqueeze(0)\n",
    "                output = model(text_encode, sentence_encode)\n",
    "                predictions.append(output.cpu().detach().numpy())\n",
    "            idx_prediction = np.argmax(predictions)\n",
    "            mean_score += scorer.score(sentences[idx_prediction], data_test[i][1])['rougeL'][2]\n",
    "    return mean_score / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "best_mean_score = 0\n",
    "for epoch in range(1000):\n",
    "    loss = train(model, dataloader, optimizer, criterion, device)\n",
    "    print('Epoch', epoch, 'Loss', loss)\n",
    "    if epoch % 100 == 0:\n",
    "        mean_score = test_validation(model, data_test, device)\n",
    "        print('Mean score', mean_score)\n",
    "        if mean_score > best_mean_score:\n",
    "            best_mean_score = mean_score\n",
    "            torch.save(model.state_dict(), 'model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
