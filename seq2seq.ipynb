{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we adapt the TD5 to our problem\n",
    "\n",
    "Of course this model is to big to learn something from a training in our small computer. We test other seq2seq models finds on differents github but even the small seq2seq model ask to much ressources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "data_encoding = 'utf-8'\n",
    "data_train_ = []\n",
    "with open('data/train.csv', 'r', encoding=data_encoding) as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        data_train_.append(row)\n",
    "\n",
    "data_test_ = []\n",
    "with open('data/validation.csv', 'r', encoding=data_encoding) as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        data_test_.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "min_freq = 5\n",
    "\n",
    "data_x_train = [re.findall(r'\\w+', row[0].lower()) for row in data_train_]\n",
    "data_y_train = [re.findall(r'\\w+', row[1].lower()) for row in data_train_]\n",
    "\n",
    "data_x_test = [re.findall(r'\\w+', row[0].lower()) for row in data_test_]\n",
    "data_y_test = [re.findall(r'\\w+', row[1].lower()) for row in data_test_]\n",
    "\n",
    "\n",
    "tokens_x_train = [token for sublist in data_x_train for token in sublist]\n",
    "tokens_y_train = [token for sublist in data_y_train for token in sublist]\n",
    "\n",
    "tokens_x_test = [token for sublist in data_x_test for token in sublist]\n",
    "tokens_y_test = [token for sublist in data_y_test for token in sublist]\n",
    "\n",
    "\n",
    "counts = dict(Counter(tokens_x_train + tokens_y_train + tokens_x_test + tokens_y_test))\n",
    "counts = {word: count for word, count in counts.items() if count >= min_freq}\n",
    "sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "# assign to each word an index based on its frequency in the corpus\n",
    "# the most frequent word will get index equal to 1\n",
    "# 0 is reserved for out-of-vocabulary words\n",
    "word_to_index = dict([(my_tuple[0],idx) for idx,my_tuple in enumerate(sorted_counts,1)])\n",
    "#reduire la taille du vocabulaire\n",
    "word_to_index = {k: v for k, v in word_to_index.items() if v < 3000}\n",
    "\n",
    "padding_token=0,\n",
    "oov_token=1,\n",
    "sos_token=2,\n",
    "eos_token=3,\n",
    "\n",
    "# add special tokens\n",
    "#decalage des index \n",
    "word_to_index = {word: index + 4 for word, index in word_to_index.items()}\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<OOV>'] = 1\n",
    "word_to_index['<SOS>'] = 2\n",
    "word_to_index['<EOS>'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to a list of lists of indices\n",
    "data_x_idx_train = [[word_to_index.get(token, 0) for token in row] for row in data_x_train]\n",
    "data_y_idx_train = [[word_to_index.get(token, 0) for token in row] for row in data_y_train]\n",
    "\n",
    "data_x_idx_test = [[word_to_index.get(token, 0) for token in row] for row in data_x_test]\n",
    "data_y_idx_test = [[word_to_index.get(token, 0) for token in row] for row in data_y_test]\n",
    "\n",
    "print(data_x_idx_train[0])\n",
    "print(data_y_idx_train[0])\n",
    "\n",
    "pair_point_train = [[data_x_idx_train[i], data_y_idx_train[i]] for i in range(len(data_x_idx_train))]\n",
    "pair_point_test = [[data_x_idx_test[i], data_y_idx_test[i]] for i in range(len(data_x_idx_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "  def __len__(self):\n",
    "        return len(self.pairs) # total nb of observations\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "        source, target = self.pairs[idx] # one observation\n",
    "        return torch.LongTensor(source), torch.LongTensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    to be passed the entire source sequence at once\n",
    "    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n",
    "    https://pytorch.org/docs/stable/nn.html#gru\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # fill the gaps # (transform input into embeddings and pass embeddings to RNN)\n",
    "        # you should return a tensor of shape (seq, batch, feat)\n",
    "        embedded = self.embedding(input)  # embed the input sequence\n",
    "        output, hidden = self.rnn(embedded)  # pass embeddings through the GRU\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''to be used one timestep at a time\n",
    "       see https://pytorch.org/docs/stable/nn.html#gru'''\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
    "        self.ff_concat = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.predict = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input, source_context, h):\n",
    "        # fill the gaps #\n",
    "        # transform input into embeddings, pass embeddings to RNN, concatenate with source_context and apply tanh, and make the prediction\n",
    "        # prediction should be of shape (1, batch, vocab), h and tilde_h of shape (1, batch, feat)\n",
    "        embedded = self.embedding(input)  # embed the input word\n",
    "        output, h_t = self.rnn(embedded, h)\n",
    "        concatenated = torch.cat([source_context, h_t], dim=2)\n",
    "        tilde_h_t = torch.tanh(self.ff_concat(concatenated))\n",
    "        prediction =self.predict(tilde_h_t)\n",
    "        return prediction, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seqAtt(nn.Module):\n",
    "    '''\n",
    "    concat global attention a la Luong et al. 2015 (subsection 3.1)\n",
    "    https://arxiv.org/pdf/1508.04025.pdf\n",
    "    '''\n",
    "    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n",
    "        super(seq2seqAtt, self).__init__()\n",
    "        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t, hidden_dim)\n",
    "        self.ff_score = nn.Linear(hidden_dim, 1, bias=False) # just a dot product here\n",
    "\n",
    "    def forward(self, target_h, source_hs):\n",
    "        target_h_rep = target_h.repeat(source_hs.size(0), 1, 1) # (1, batch, feat) -> (seq, batch, feat)\n",
    "        # fill the gaps #\n",
    "        # implement the score computation part of the concat formulation (see section 3.1. of Luong 2015)\n",
    "        concat_output = torch.cat((source_hs, target_h_rep), dim=2)\n",
    "        scores =  self.ff_score(torch.tanh(self.ff_concat(concat_output))) # should be of shape (seq, batch, 1)\n",
    "        scores = scores.squeeze(dim=2) # (seq, batch, 1) -> (seq, batch). dim = 2 because we don't want to squeeze the batch dim if batch size = 1\n",
    "        norm_scores = torch.softmax(scores, 0)\n",
    "        source_hs_p = source_hs.permute((2, 0, 1)) # (seq, batch, feat) -> (feat, seq, batch)\n",
    "        weighted_source_hs = (norm_scores * source_hs_p) # (seq, batch) * (feat, seq, batch) (* checks from right to left that the dimensions match)\n",
    "        ct = torch.sum(weighted_source_hs.permute((1, 2, 0)), 0, keepdim=True) # (feat, seq, batch) -> (seq, batch, feat) -> (1, batch, feat); keepdim otherwise sum squeezes\n",
    "        return ct, norm_scores.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seqModel(nn.Module):\n",
    "    '''the full seq2seq model'''\n",
    "    ARGS = ['vocab_s','source_language','vocab_t_inv','embedding_dim_s','embedding_dim_t',\n",
    "     'hidden_dim_s','hidden_dim_t','hidden_dim_att','do_att','padding_token',\n",
    "     'oov_token','sos_token','eos_token','max_size']\n",
    "    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t,\n",
    "                 hidden_dim_s, hidden_dim_t, hidden_dim_att, do_att, padding_token,\n",
    "                 oov_token, sos_token, eos_token, max_size):\n",
    "        super(seq2seqModel, self).__init__()\n",
    "        self.vocab_s = vocab_s\n",
    "        self.source_language = source_language\n",
    "        self.vocab_t_inv = vocab_t_inv\n",
    "        self.embedding_dim_s = embedding_dim_s\n",
    "        self.embedding_dim_t = embedding_dim_t\n",
    "        self.hidden_dim_s = hidden_dim_s\n",
    "        self.hidden_dim_t = hidden_dim_t\n",
    "        self.hidden_dim_att = hidden_dim_att\n",
    "        self.do_att = do_att # should attention be used?\n",
    "        self.padding_token = padding_token\n",
    "        self.oov_token = oov_token\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.max_size = max_size\n",
    "\n",
    "        self.max_source_idx = max(list(vocab_s.values()))\n",
    "        print('max source index',self.max_source_idx)\n",
    "        print('source vocab size',len(vocab_s))\n",
    "\n",
    "        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n",
    "        print('max target index',self.max_target_idx)\n",
    "        print('target vocab size',len(vocab_t_inv))\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.encoder = Encoder(self.max_source_idx+1, self.embedding_dim_s, self.hidden_dim_s, self.padding_token).to(self.device)\n",
    "        self.decoder = Decoder(self.max_target_idx+1, self.embedding_dim_t, self.hidden_dim_t, self.padding_token).to(self.device)\n",
    "\n",
    "        if self.do_att:\n",
    "            self.att_mech = seq2seqAtt(self.hidden_dim_att, self.hidden_dim_s, self.hidden_dim_t).to(self.device)\n",
    "\n",
    "    def my_pad(self, my_list):\n",
    "        '''my_list is a list of tuples of the form [(tensor_s_1, tensor_t_1), ..., (tensor_s_batch, tensor_t_batch)]\n",
    "        the <eos> token is appended to each sequence before padding\n",
    "        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence'''\n",
    "        batch_source = pad_sequence([torch.cat((elt[0], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
    "        batch_target = pad_sequence([torch.cat((elt[1], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
    "        return batch_source, batch_target\n",
    "\n",
    "    def forward(self, input, max_size, is_prod):\n",
    "        if is_prod:\n",
    "            input = input.unsqueeze(1) # (seq) -> (seq, 1) 1D input <=> we receive just one sentence as input (predict/production mode)\n",
    "        current_batch_size = input.size(1)\n",
    "        # fill the gap #\n",
    "        # use the encoder\n",
    "        source_hs = self.encoder(input)\n",
    "        # = = = decoder part (one timestep at a time)  = = =\n",
    "        target_h = torch.zeros(size=(1, current_batch_size, self.hidden_dim_t)).to(self.device) # init (1, batch, feat)\n",
    "\n",
    "        # fill the gap #\n",
    "        # (initialize target_input with the proper token)\n",
    "        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # init (1, batch)\n",
    "        pos = 0\n",
    "        eos_counter = 0\n",
    "        logits = []\n",
    "        weights = []\n",
    "\n",
    "        while True:\n",
    "            if self.do_att:\n",
    "                source_context = self.att_mech(target_h, source_hs)\n",
    "\n",
    "                weights.append(source_context[1])\n",
    "                # FILL THE GAP FOR QUESTION 3 (Not Necessary for TASK4): fill \"weights\" variable\n",
    "\n",
    "                source_context =  source_context[0] # (1, batch, feat)\n",
    "            else:\n",
    "                source_context = source_hs[-1, :, :].unsqueeze(0) # (1, batch, feat) last hidden state of encoder\n",
    "            # fill the gap #\n",
    "            # use the decoder\n",
    "            prediction, target_h = self.decoder(target_input, source_context, target_h)\n",
    "            logits.append(prediction) # (1, batch, vocab)\n",
    "            # fill the gap #\n",
    "            # get the next input to pass the decoder\n",
    "            target_input = prediction.argmax(dim=2)  # Greedy decoding\n",
    "\n",
    "            eos_counter += torch.sum(target_input==self.eos_token).item()\n",
    "            pos += 1\n",
    "            if pos >= max_size or (eos_counter == current_batch_size and is_prod):\n",
    "                break\n",
    "        to_return = torch.cat(logits, 0) # logits is a list of tensors -> (seq, batch, vocab)\n",
    "\n",
    "        if is_prod:\n",
    "            to_return = to_return.squeeze(dim=1) # (seq, vocab)\n",
    "\n",
    "        return to_return, weights\n",
    "\n",
    "    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n",
    "        parameters = [p for p in self.parameters() if p.requires_grad]\n",
    "        optimizer = optim.Adam(parameters, lr=lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n",
    "        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "        # we pass a collate function to perform padding on the fly, within each batch\n",
    "        # this is better than truncation/padding at the dataset level\n",
    "        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size,\n",
    "                                       shuffle=True, collate_fn=self.my_pad) # returns (batch, seq)\n",
    "        test_loader = data.DataLoader(testDataset, batch_size=512,\n",
    "                                      collate_fn=self.my_pad)\n",
    "        tdqm_dict_keys = ['loss', 'test loss']\n",
    "        tdqm_dict = dict(zip(tdqm_dict_keys, [0.0, 0.0]))\n",
    "        patience_counter = 1\n",
    "        patience_loss = 99999\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            with tqdm(total=len(train_loader), unit_scale=True, postfix={'loss':0.0, 'test loss':0.0},\n",
    "                      desc=\"Epoch : %i/%i\" % (epoch, n_epochs-1), ncols=100) as pbar:\n",
    "                for loader_idx, loader in enumerate([train_loader, test_loader]):\n",
    "                    total_loss = 0\n",
    "                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "                    if loader_idx == 0:\n",
    "                        self.train()\n",
    "                    else:\n",
    "                        self.eval()\n",
    "                    for i, (batch_source, batch_target) in enumerate(loader):\n",
    "                        batch_source = batch_source.transpose(1, 0).to(self.device) # RNN needs (seq, batch, feat) but loader returns (batch, seq)\n",
    "                        batch_target = batch_target.transpose(1, 0).to(self.device) # (seq, batch)\n",
    "\n",
    "                        # are we using the model in production\n",
    "                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq, batch), i.e., train or test\n",
    "                        if is_prod:\n",
    "                            max_size = self.max_size\n",
    "                            self.eval()\n",
    "                        else:\n",
    "                            max_size = batch_target.size(0) # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n",
    "\n",
    "                        unnormalized_logits = self.forward(batch_source, max_size, is_prod)[0]\n",
    "                        sentence_loss = criterion(unnormalized_logits.flatten(end_dim=1), batch_target.flatten())\n",
    "                        total_loss += sentence_loss.item()\n",
    "                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss/(i+1)\n",
    "                        pbar.set_postfix(tdqm_dict)\n",
    "                        if loader_idx == 0:\n",
    "                            optimizer.zero_grad() # flush gradient attributes\n",
    "                            sentence_loss.backward() # compute gradients\n",
    "                            optimizer.step() # update\n",
    "                            pbar.update(1)\n",
    "\n",
    "            if total_loss > patience_loss:\n",
    "                patience_counter += 1\n",
    "            else:\n",
    "                patience_loss = total_loss\n",
    "                patience_counter = 1 # reset\n",
    "\n",
    "            if patience_counter > patience:\n",
    "                break\n",
    "\n",
    "    def sourceNl_to_ints(self, source_nl):\n",
    "        '''converts natural language source sentence into source integers'''\n",
    "        source_nl_clean = source_nl.lower().replace(\"'\",' ').replace('-',' ')\n",
    "        source_nl_clean_tok = word_tokenize(source_nl_clean, self.source_language)\n",
    "        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n",
    "                       self.oov_token for elt in source_nl_clean_tok]\n",
    "\n",
    "        source_ints = torch.LongTensor(source_ints).to(self.device)\n",
    "        return source_ints\n",
    "\n",
    "    def targetInts_to_nl(self, target_ints):\n",
    "        '''converts integer target sentence into target natural language'''\n",
    "        return ['<PAD>' if elt==self.padding_token else '<OOV>' if elt==self.oov_token \\\n",
    "                else '<EOS>' if elt==self.eos_token else '<SOS>' if elt==self.sos_token\\\n",
    "                else self.vocab_t_inv[elt] for elt in target_ints]\n",
    "\n",
    "    def predict(self, source_nl):\n",
    "        source_ints = self.sourceNl_to_ints(source_nl)\n",
    "        logits = self.forward(source_ints, self.max_size, True) # (seq) -> (<=max_size, vocab)\n",
    "        target_ints = logits[0].argmax(-1).squeeze() # (<=max_size, 1) -> (<=max_size)\n",
    "        target_nl = self.targetInts_to_nl(target_ints.tolist())\n",
    "        return ' '.join(target_nl), logits[1]\n",
    "\n",
    "    def save(self, path_to_file):\n",
    "        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n",
    "        attrs['state_dict'] = self.state_dict()\n",
    "        torch.save(attrs, path_to_file)\n",
    "\n",
    "    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n",
    "    def load(cls, path_to_file):\n",
    "        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n",
    "        state_dict = attrs.pop('state_dict')\n",
    "        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n",
    "        new.load_state_dict(state_dict)\n",
    "        return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_att = True # should always be set to True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_set = Dataset(pair_point_train)\n",
    "test_set = Dataset(pair_point_test)\n",
    "\n",
    "vocab_target_inv = {v:k for k,v in word_to_index.items()}\n",
    "vocab_source = {k:v for k,v in word_to_index.items()}\n",
    "\n",
    "model = seq2seqModel(vocab_s=vocab_source,\n",
    "                      source_language='english',\n",
    "                      vocab_t_inv=vocab_target_inv,\n",
    "                      embedding_dim_s=20,\n",
    "                      embedding_dim_t=20,\n",
    "                      hidden_dim_s=10,\n",
    "                      hidden_dim_t=10,\n",
    "                      hidden_dim_att=10,\n",
    "                      do_att=do_att,\n",
    "                      padding_token=0,\n",
    "                      oov_token=1,\n",
    "                      sos_token=2,\n",
    "                      eos_token=3,\n",
    "                      max_size=15).to(device)\n",
    "\n",
    "model.fit(training_set,test_set,lr=0.001,batch_size=64,n_epochs=20,patience=2)\n",
    "model.save('my_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
