{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "# Initialize Rouge Scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'])\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoding = 'utf-8'\n",
    "\n",
    "data_test_ = []\n",
    "with open('data/validation.csv', 'r', encoding=data_encoding) as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        data_test_.append(row)\n",
    "\n",
    "data_test = data_test_[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First idea : use the first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rouge-L Score:  0.1535873817959459\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = []\n",
    "\n",
    "# Function that generates summaries using LEAD-N\n",
    "def lead_summary(text):\n",
    "    sentences = text.split('.')\n",
    "    return sentences[0] + '.'\n",
    "\n",
    "for i in range(len(data_test)):\n",
    "    row = data_test[i][0]\n",
    "    summary = lead_summary(row)\n",
    "    scores = scorer.score(summary, data_test[i][1])['rougeL'][2]\n",
    "    rouge_scores.append(scores)\n",
    "\n",
    "print(\"Average Rouge-L Score: \", sum(rouge_scores)/len(rouge_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second idea : use td-idf to find an important sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16814216784723096"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "def reformulate_sentences(text, top_words, window_size=2):\n",
    "    # Tokenizing the sentences\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    \n",
    "    reformulated_sentences = []\n",
    "    sentence_positions = []\n",
    "    total_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Checking if the sentence contains any of the important words\n",
    "        if any(word in sentence.lower() for word, _ in top_words):\n",
    "            # Extracting words from the sentence\n",
    "            words = sentence.split()\n",
    "            # Finding indices of important words in the sentence\n",
    "            important_word_indices = [i for i, word in enumerate(words) if word.lower() in [word for word, _ in top_words]]\n",
    "            # Reformulating the sentence by highlighting the important words\n",
    "            for index in important_word_indices:\n",
    "                start_index = max(0, index - window_size)\n",
    "                end_index = min(len(words), index + window_size + 1)\n",
    "                reformulated_sentence = ' '.join(words[start_index:end_index])\n",
    "                reformulated_sentences.append(reformulated_sentence)\n",
    "                sentence_positions.append(total_length + index)\n",
    "            # Removing words already processed\n",
    "            top_words = [(word, score) for word, score in top_words if word.lower() not in sentence.lower()]\n",
    "\n",
    "            total_length += len(words)\n",
    "\n",
    "    # Cleaning to avoid redundancies\n",
    "    fusion_pre = ' '.join(reformulated_sentences)\n",
    "    words = fusion_pre.split()\n",
    "    words = [word.lower() for word in words]\n",
    "    words = list(set(words))\n",
    "    words_flag = {word: 0 for word in words}\n",
    "    fusion = ''\n",
    "    for word in fusion_pre.split():\n",
    "        if words_flag[word.lower()] == 0:\n",
    "            fusion += word + ' '\n",
    "            words_flag[word.lower()] = 1\n",
    "    \n",
    "    return fusion\n",
    "\n",
    "def gather_previous_code(data_train, window_size=2, number_words=10):\n",
    "    score_average = 0\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit([x[0] for x in data_train])\n",
    "    for text, summary in data_train:\n",
    "        tokens = word_tokenize(text.lower()) \n",
    "        tokens = [word for word in tokens if word.isalnum() and word not in stopwords.words('english')]\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        tfidf_matrix = vectorizer.transform([cleaned_text])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = tfidf_matrix.toarray().flatten()\n",
    "        word_scores = list(zip(feature_names, tfidf_scores))\n",
    "        sorted_word_scores = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
    "        top_words = sorted_word_scores[:number_words]\n",
    "        fusion = reformulate_sentences(text, top_words, window_size)\n",
    "        score = scorer.score(fusion, summary)['rougeL'][2]\n",
    "        score_average += score\n",
    "        best_score = 0\n",
    "        \n",
    "        for sentence in re.split(r'[.!?]', text):\n",
    "            score = scorer.score(sentence, summary)['rougeL'][2]\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "    return score_average / len(data_train)\n",
    "\n",
    "gather_previous_code(data_test[:100], window_size=4, number_words=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# third idea : use TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B/glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sen):\n",
    "    stop_words = stopwords.words('french')\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "\n",
    "def rank_sentences(text, word_embeddings):\n",
    "    sentences = re.split(r'[.]', text)\n",
    "\n",
    "    # remove punctuations, numbers and special characters\n",
    "    clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "    # make alphabets lowercase\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "    sentence_vectors = []\n",
    "    for i in clean_sentences:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        sentence_vectors.append(v)\n",
    "    # similarity matrix\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "    try:\n",
    "        nx_graph = nx.from_numpy_array(sim_mat)\n",
    "        scores = nx.pagerank(nx_graph, max_iter=1000)\n",
    "        #ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "        ranked_sentences = ((scores[i],s) for i,s in enumerate(sentences))\n",
    "        # Extract top 10 sentences as the summary\n",
    "        return ranked_sentences, True\n",
    "    except:\n",
    "        return [], False\n",
    "    \n",
    "for i in range(5):\n",
    "    text = data_test[i][0]\n",
    "    title = data_test[i][1]\n",
    "    ranked_sentences = rank_sentences(text, word_embeddings)\n",
    "    for i in range(5):\n",
    "        if len(ranked_sentences) > i:\n",
    "            score_ranked = scorer.score(title, ranked_sentences[i][1])['rougeL'][2]\n",
    "            print(score_ranked)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
